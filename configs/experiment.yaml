# Experiment configuration for VisDrone robustness evaluation

# Reproducibility
seed: 42

# Dataset paths
dataset:
  visdrone_root: "datasets/visdrone"
  visdrone_yolo_root: "datasets/visdrone_yolo"
  corruptions_root: "datasets/visdrone_corrupt"
  
# VisDrone download
visdrone:
  url: "https://github.com/VisDrone/VisDrone-Dataset"
  # Will use direct download links for VisDrone2019-DET
  train_url: "http://aiskyeye.com/download/object-detection-2/VisDrone2019-DET-train.zip"
  val_url: "http://aiskyeye.com/download/object-detection-2/VisDrone2019-DET-val.zip"
  test_url: "http://aiskyeye.com/download/object-detection-2/VisDrone2019-DET-test-dev.zip"
  
# Tiny object definition
# Based on statistical analysis: 10th percentile of bbox areas in VisDrone dataset
# See tiny_object_recommendation.md for detailed analysis and rationale
tiny_objects:
  area_threshold: 60  # pixels^2 (10th percentile, covers 10.26% of bboxes)
  width_threshold: 6  # pixels (10th percentile)
  height_threshold: 9  # pixels (10th percentile)
  sample_size: 500  # number of GT boxes to sample
  
# Corruptions
corruptions:
  types: ["fog", "lowlight", "motion_blur"]
  severities: [0, 1, 2, 3, 4]
  fog:
    alpha: [0.0, 0.15, 0.30, 0.45, 0.60]  # fog strength
  lowlight:
    gamma: [1.0, 1.4, 1.8, 2.2, 2.6]
    brightness_scale: [1.0, 0.85, 0.70, 0.55, 0.40]
  motion_blur:
    kernel_length: [0, 7, 13, 19, 25]
    angle: 0  # degrees, or use hash-based per image
    
# Models
models:
  yolo_generic:
    type: "yolo"
    architecture: "yolov8s"
    pretrained: "yolov8s.pt"
    fine_tuned: false
    checkpoint: null
    
  yolo_ft:
    type: "yolo"
    architecture: "yolov8s"
    pretrained: "yolov8s.pt"
    fine_tuned: true
    checkpoint: "results/models/yolo_ft_visdrone.pt"
    train_epochs: 100
    train_imgsz: 640
    train_batch: 16
    
  rt_detr:
    type: "rtdetr"
    architecture: "rtdetr-l"
    pretrained: "rtdetr-l.pt"
    fine_tuned: false
    checkpoint: null
    
# Inference settings
inference:
  imgsz: 640
  conf_thres: 0.25
  iou_thres: 0.45
  
# Frame sequence settings
frame_sequences:
  min_clip_length: 5  # minimum frames per clip
  max_clip_length: 30  # maximum frames per clip
  clip_stride: 5  # stride for extracting clips
  
# Evaluation
evaluation:
  splits: ["val"]  # primary split
  iou_thresholds: [0.5, 0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95]  # for mAP@0.5:0.95
  tiny_match_iou_threshold: 0.5
  tiny_match_same_class: true
  
# Risk region detection (automatic identification)
risk_detection:
  map_drop_threshold: 0.15  # absolute drop from severity 0
  miss_rate_threshold: 0.5  # miss rate threshold for risk region
  score_drop_threshold: 0.2  # score drop threshold
  iou_drop_threshold: 0.2  # IoU drop threshold
  instability_threshold: 0.3  # variance threshold for instability detection
  window_size: 5  # frames before failure event for CAM analysis
  
# Grad-CAM (failure-event based)
gradcam:
  enabled: true
  target_layer: "model.model[-2]"  # last conv layer in YOLO
  top_k_percent: 10
  save_samples: true
  max_samples: 20
  failure_window: 5  # W frames before failure event
  metrics: ["energy_in_bbox", "activation_spread", "entropy", "center_shift"]  # CAM distribution metrics
  
# Results paths
results:
  root: "results"
  metrics_csv: "results/metrics_dataset.csv"
  tiny_curves_csv: "results/tiny_curves.csv"
  tiny_records_csv: "results/tiny_records.csv"
  gradcam_metrics_csv: "results/gradcam_metrics.csv"
  report_md: "results/report.md"
  
# LLM report
llm_report:
  model: "gpt-4o-mini"
  temperature: 0.0
  max_tokens: 4000
